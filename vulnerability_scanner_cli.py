import requests
from urllib.parse import urlencode, urljoin, urlparse
from bs4 import BeautifulSoup
import os
import time

# Banner for CLI interface
def display_banner():
    banner = """
==================================
      Vulnerability-Scanner
==================================
    """
    print(banner)

# Function to crawl web pages up to a given depth
def web_crawler(base_url, max_depth):
    visited = set()
    to_visit = [(base_url, 0)]  # List of URLs to visit along with their depth
    discovered_urls = []

    while to_visit:
        url, depth = to_visit.pop(0)
        if url in visited or depth > max_depth:
            continue

        try:
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            visited.add(url)
            discovered_urls.append(url)
            print(f"[Crawled] Discovered URL: {url}")

            if depth < max_depth:
                for link in soup.find_all('a', href=True):
                    full_url = urljoin(base_url, link['href'])
                    parsed_url = urlparse(full_url)
                    if parsed_url.netloc == urlparse(base_url).netloc and full_url not in visited:
                        to_visit.append((full_url, depth + 1))

        except requests.exceptions.RequestException as e:
            print(f"[Error] Unable to crawl {url}: {e}")
            continue

    return discovered_urls

# Function to scan for SQL Injection
def scan_sql_injection(url):
    print(f"  [Testing] SQL Injection for: {url}")
    payloads = ["' OR '1'='1", "' OR 'a'='a", "1' AND SLEEP(5)-- ", "' UNION SELECT NULL, NULL-- "]
    results = []

    for payload in payloads:
        full_url = f"{url}?{urlencode({'input': payload})}"
        try:
            response = requests.get(full_url, timeout=10)
            if any(error in response.text.lower() for error in ["syntax error", "mysql_fetch", "sql syntax"]):
                results.append(f"Potential SQL Injection detected with payload: {payload}")
            if response.elapsed.total_seconds() > 5:
                results.append(f"Potential Time-based Blind SQL Injection detected with payload: {payload}")
        except requests.exceptions.RequestException as e:
            print(f"    [Error] SQL Injection scan failed for {full_url}: {e}")

    if not results:
        results.append("No SQL Injection vulnerabilities found.")
    return results

# Function to scan for XSS
def scan_xss(url):
    print(f"  [Testing] XSS for: {url}")
    payloads = [
        '<script>alert(1)</script>',
        '"><script>alert(1)</script>',
        '<img src=x onerror=alert(1)>',
        '<svg onload=alert(1)>',
        '<body/onload=alert(1)>',
        '<iframe src="javascript:alert(1)">'
    ]
    results = []

    for payload in payloads:
        full_url = f"{url}?{urlencode({'input': payload})}"
        try:
            response = requests.get(full_url, timeout=10)
            if payload in response.text:
                results.append(f"Potential XSS detected with payload: {payload}")
        except requests.exceptions.RequestException as e:
            print(f"    [Error] XSS scan failed for {full_url}: {e}")

    if not results:
        results.append("No XSS vulnerabilities found.")
    return results

# Function to check for insecure headers
def check_insecure_headers(url):
    print(f"  [Testing] Insecure Headers for: {url}")
    try:
        response = requests.get(url, timeout=10)
        headers = response.headers
        insecure_headers = []

        required_headers = {
            "Content-Security-Policy": "Not Present",
            "X-Frame-Options": "Not Present",
            "X-XSS-Protection": "Not Present",
            "Strict-Transport-Security": "Not Present",
            "Referrer-Policy": "Not Present",
            "Permissions-Policy": "Not Present",
            "Expect-CT": "Not Present",
            "Cache-Control": "Not Present"
        }

        for header, status in required_headers.items():
            if header not in headers:
                insecure_headers.append(f"Missing security header: {header}")
            else:
                if header == "Content-Security-Policy" and "unsafe-inline" in headers[header]:
                    insecure_headers.append(f"Insecure Content-Security-Policy header: {headers[header]}")
                if header == "X-Frame-Options" and headers[header] not in ["DENY", "SAMEORIGIN"]:
                    insecure_headers.append(f"Insecure X-Frame-Options header: {headers[header]}")

        if not insecure_headers:
            insecure_headers.append("All important security headers are present.")
        
        return insecure_headers

    except requests.exceptions.RequestException as e:
        print(f"    [Error] Failed to check headers for {url}: {e}")
        return []

# Function to run all checks on crawled URLs
def run_scan(url, max_depth):
    results = {}

    # Step 1: Crawl the website
    print(f"\n[INFO] Starting crawl on {url} with depth {max_depth}...")
    discovered_urls = web_crawler(url, max_depth)
    total_urls = len(discovered_urls)
    
    # Step 2: Perform tests on each discovered URL
    for i, target_url in enumerate(discovered_urls):
        print(f"\n[INFO] Scanning {target_url} ({i + 1}/{total_urls})")
        
        # Initialize vulnerability results for the current URL
        results[target_url] = {
            'SQL Injection': [],
            'XSS': [],
            'Insecure Headers': []
        }

        # SQL Injection Scan
        sql_injection_results = scan_sql_injection(target_url)
        results[target_url]['SQL Injection'] = sql_injection_results

        # XSS Scan
        xss_results = scan_xss(target_url)
        results[target_url]['XSS'] = xss_results

        # Insecure Headers Check
        insecure_headers_results = check_insecure_headers(target_url)
        results[target_url]['Insecure Headers'] = insecure_headers_results

    return results

# Function to display and save results
def display_results(scan_results):
    # Step 3: Show results in a more human-readable format
    print("\n[RESULTS] Vulnerability Scan Summary")
    for url, vulnerabilities in scan_results.items():
        print(f"\n[INFO] Scanning Results for: {url}")
        
        for category, findings in vulnerabilities.items():
            print(f"\n  {category} Results:")
            if findings:
                for finding in findings:
                    print(f"    - {finding}")
            else:
                print(f"    - No {category} vulnerabilities found.")
    
    # Step 4: Ask the user to save results
    save_to_file = input("\n[INFO] Do you want to save the results to a file? (yes/no): ").strip().lower()
    
    if save_to_file == 'yes':
        filename = input("Enter the filename to save the results (e.g., scan_results.txt): ").strip()
        with open(filename, 'w') as f:
            for url, vulnerabilities in scan_results.items():
                f.write(f"\nScanning Results for: {url}\n")
                for category, findings in vulnerabilities.items():
                    f.write(f"\n  {category} Results:\n")
                    if findings:
                        for finding in findings:
                            f.write(f"    - {finding}\n")
                    else:
                        f.write(f"    - No {category} vulnerabilities found.\n")
        print(f"[INFO] Results saved to {os.path.abspath(filename)}")
    else:
        print("[INFO] Results not saved.")

# Main function to run the CLI
def main():
    # Display the CLI banner
    display_banner()

    # Collect URL and depth input from user
    url = input("Enter the target URL: ").strip()
    while True:
        try:
            max_depth = int(input("Enter the crawl depth (e.g., 3): ").strip())
            break
        except ValueError:
            print("Please enter a valid integer for depth.")
    
    # Start the scan
    scan_results = run_scan(url, max_depth)

    # Display and save the scan results
    display_results(scan_results)

if __name__ == "__main__":
    main()
